from keras.layers import *
from lib.wrappers import *
from keras.regularizers import l2
from keras.optimizers import RMSprop, Adagrad
from keras.constraints import maxnorm
from keras.layers.core import Lambda
from keras.models import Model
import theano
import theano.tensor as T
import numpy as np
import sys
from keras import backend as K

def lossFn(y_true, y_pred):
    return T.mean(y_pred)

def cross_e1_e2(X):
    e1 = X[:,0]
    e2 = X[:,1]
    return e1*e2

def get_cross(i, neg_samples):
    def cross_fn(entity_vecs, entity_negative_vecs):
        ei = entity_vecs[:,i]
        id_ei = entity_negative_vecs[:, i*neg_samples : (i+1)*neg_samples]
        return ei.dimshuffle(0,'x',1)*id_ei
    return lambda X: cross_fn(X[0], X[1])

def get_dot(i):
    def dot_fn(relation_vecs, entity_vecs):
        ei = entity_vecs[:,i]
        dotProd = T.batched_dot(relation_vecs, ei)
        return dotProd
    return lambda X: dot_fn(X[0], X[1])

def get_dot_neg(i, neg_samples):
    def dot_fn(relation_vecs, entity_negative_vecs):
        id_ei = entity_negative_vecs[:, i*neg_samples: (i+1)*neg_samples]
        return T.batched_dot(id_ei,relation_vecs)
    return lambda X: dot_fn(X[0], X[1]) 


def get_forward_pass(layers):   
    def run(input):
        output = layers[0](input)

        if len(layers)>1:
            for i in xrange(1, len(layers)):
                output = layers[i](output)

        return output

    return run

def get_softmax_approx(input):
    score_combined, score_combined_e2_corrupt = input
    ''' f(e1, r, e2) = r.T(e1*e2) '''
    '''denom_e1 = sum{j=1..200}exp(f(e1, r, e2j)) where e2j is a negative sample '''
    max_denom_e2 = T.max(score_combined_e2_corrupt, axis = 1, keepdims=True)
    denom_e2 = T.exp(score_combined_e2_corrupt - max_denom_e2).sum(axis=1)

    numer = score_combined - max_denom_e2.dimshuffle(0)
    net_score= numer - T.log(denom_e2)
    return -1*net_score

def get_logistic(input):
    print "I am here! - get_logistic",len(input)
    if len(input) ==3:
        score, score_e1_corrupt, score_e2_corrupt = input
    else:
        score, score_e1_corrupt = input
    loss_score = T.nnet.softplus(-1*score)
    loss_e1_corrupt = T.sum(T.nnet.softplus(1*score_e1_corrupt), axis=1)
    
    if len(input) ==3:
        loss_e2_corrupt = T.sum(T.nnet.softplus(1*score_e2_corrupt), axis=1)
        net_loss = loss_score + loss_e1_corrupt + loss_e2_corrupt
        net_loss = net_loss/(1+(2)*score_e1_corrupt.shape[1])
    else:
        net_loss = loss_score + loss_e1_corrupt 
        net_loss = net_loss/(1+(1)*score_e1_corrupt.shape[1])
    return 1*net_loss

def inner_prod(x1, x2, x3):
    return T.batched_dot(x1*x2, x3)

def hermitian_product(X):
    e_real, e_im, r_real, r_im = X
    e1_real = e_real[:, 0]; e2_real = e_real[:,1]
    e1_im = e_im[:, 0]; e2_im = e_im[:, 1]
    return inner_prod(e1_real, e2_real, r_real) + inner_prod(e1_im, e2_im, r_real) + inner_prod(e1_real, e2_im, r_im) - inner_prod(e1_im, e2_real, r_im)

def getE_score_joint(kb_entities, kb_relations, neg_samples_kb, opts):
    neg_samples   = opts.neg_samples
    vect_dim      = opts.vect_dim #19
    num_entities  = opts.num_entities
    num_relations = opts.num_relations
    if opts.l2_entity_E:
    	l2_reg_entities = opts.l2_entity_E
    else:
	    l2_reg_relations = opts.l2_relation
    entities    = Embedding(output_dim=vect_dim, input_dim=num_entities+1, init='normal',name = 'entity_embeddings')#,  W_regularizer=l2(l2_reg_entities))
    relations_s = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_s', W_regularizer=l2(l2_reg_relations))
    relations_o = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_o', W_regularizer=l2(l2_reg_relations))

    entity_vectors = entities(kb_entities)
    entity_negative_vectors = entities(neg_samples_kb)
    relation_vectors_s = Flatten()(relations_s(kb_relations))
    relation_vectors_o = Flatten()(relations_o(kb_relations))

    get_dot_1 =  get_dot(0)
    get_dot_2 =  get_dot(1)
    get_dot_neg_1 = get_dot_neg(0, neg_samples)
    rs_dot_e1 = merge([relation_vectors_s, entity_vectors], mode =get_dot_1, output_shape = ())
    ro_dot_e2 = merge([relation_vectors_o, entity_vectors], mode =get_dot_2, output_shape = ())
    ro_dot_e2_prime = merge([relation_vectors_o, entity_negative_vectors], mode =get_dot_neg_1, output_shape = (neg_samples,))

    score_E = merge([rs_dot_e1, ro_dot_e2], mode = lambda X : X[0]+X[1], output_shape=())

    score_E_e2_corrupted = merge([rs_dot_e1, ro_dot_e2_prime], mode = lambda X: X[0].dimshuffle(0,'x') + X[1], output_shape=(neg_samples,))

    if opts.add_loss:
        get_dot_neg_2 = get_dot_neg(1, neg_samples)
        rs_dot_e1_prime = merge([relation_vectors_s, entity_negative_vectors], mode =get_dot_neg_2, output_shape = (neg_samples,))
        score_E_e1_corrupted = merge([ro_dot_e2, rs_dot_e1_prime], mode = lambda X: X[0].dimshuffle(0,'x') + X[1], output_shape=(neg_samples,))

    else:
        score_E_e1_corrupted = None
    
    def l2_theo(input):
        entity_vectors, relation_vectors_s, relation_vectors_o, entity_negative_vectors = input
        return (T.sqr(entity_vectors).mean()) + (T.sqr(entity_negative_vectors).mean()) + (T.sqr(relation_vectors_o).mean()) + (T.sqr(relation_vectors_s).mean()) 

    if opts.theo_reg:
        reg = merge([entity_vectors, relation_vectors_s, relation_vectors_o, entity_negative_vectors], mode = l2_theo,  output_shape = ())
        return score_E, score_E_e1_corrupted, score_E_e2_corrupted, reg
    else:
        return score_E, score_E_e1_corrupted, score_E_e2_corrupted, 0


def getMF_score_joint(kb_entity_pairs, kb_relations, neg_samples_kb, relations, opts): 
    vect_dim      = opts.vect_dim
    neg_samples   = opts.neg_samples
    num_entity_pairs  = opts.num_entity_pairs
    num_relations = opts.num_relations
    l2_reg_entity_pair = opts.l2_entity_pair
    # +1 for the OOV embedding. This might change to support OOV embeddings of the form (e1, ?) instead.
    if 0:
        entity_pairs  = Embedding(output_dim=vect_dim, input_dim=num_entity_pairs+1, init='normal', name = 'entity_embeddings_MF',  W_regularizer=l2(opts.l2_relation_MF))#l2_reg_entity_pair))
        print("regularizing MF entity embeddings with l2 penalty of: %5.4f" %opts.l2_relation_MF)
    else:
        entity_pairs  = Embedding(output_dim=vect_dim, input_dim=num_entity_pairs+1, init='normal', name = 'entity_embeddings_MF')
        #entity_pairs  = Embedding(output_dim=vect_dim, input_dim=num_entity_pairs+1, init='zeros', name = 'entity_embeddings_MF', trainable = False)

    entity_pair_vectors = Flatten()(entity_pairs(kb_entity_pairs))
    entity_pair_negative_vectors = entity_pairs(neg_samples_kb)

    relation_vectors = Flatten()(relations(kb_relations))
    r_dot_e = merge([relation_vectors, entity_pair_vectors], mode = lambda X: T.batched_dot(X[0], X[1]), output_shape = ())
    r_dot_e_prime = merge([relation_vectors, entity_pair_negative_vectors], mode ='dot', output_shape = (neg_samples), dot_axes=(1,2))

    def l2_theo(input):
        entity_vectors, relation_vectors, entity_negative_vectors = input
        return (T.sqr(entity_vectors).mean()) + (T.sqr(entity_negative_vectors).mean()) + (T.sqr(relation_vectors).mean())

    if opts.theo_reg:
        reg = merge([entity_pair_vectors, relation_vectors, entity_pair_negative_vectors], mode = l2_theo,  output_shape = ())
        return r_dot_e, r_dot_e_prime, reg
    else:
        return r_dot_e, r_dot_e_prime , 0



def prepare_concat_e1_eNeg(i, neg_samples, dim, entity_vecs, entity_negative_vecs):
    id_ei = Lambda(lambda X: X[:, i*neg_samples : (i+1)*neg_samples], output_shape = (neg_samples, dim))(entity_negative_vecs)
    ei_tiled = Lambda(lambda X : T.tile(X[:,i].dimshuffle(0,'x',1), reps=(1,neg_samples,1)),output_shape = (neg_samples, dim))(entity_vecs)
    if i:
        return merge([ei_tiled, id_ei], mode = 'concat', output_shape = (neg_samples, dim*3))
    else:
        return merge([id_ei, ei_tiled], mode = 'concat', output_shape = (neg_samples, dim*3))


def getComplexDM_score_joint(kb_entities, kb_relations, neg_samples_kb, relations, opts):
    neg_samples   = opts.neg_samples
    vect_dim      = opts.vect_dim
    num_entities  = opts.num_entities
    num_relations = opts.num_relations
    l2_reg_entities = opts.l2_entity#opts.theo_reg#opts.l2_entity    #1
    l2_reg_relations = opts.l2_relation#opts.theo_reg#opts.l2_relation  #1

    def accumulator(x, y, relation_vectors, f):
        a00 = merge([x[0], y[0]], mode = f, output_shape = (neg_samples, vect_dim))
        a01 = merge([x[0], y[1]], mode = f, output_shape = (neg_samples, vect_dim))
        a11 = merge([x[1], y[1]], mode = f, output_shape = (neg_samples, vect_dim))
        a10 = merge([x[1], y[0]], mode = f, output_shape = (neg_samples, vect_dim))

        r1 = merge([relation_vectors[0], a00], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r2 = merge([relation_vectors[0], a11], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r3 = merge([relation_vectors[1], a01], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r4 = merge([relation_vectors[1], a10], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))

        result = merge([r1,r2,r3,r4], mode = lambda X : X[0]+X[1]+X[2]-X[3], output_shape=(neg_samples,))
        return result

    def accumulator_e2(x, y, relation_vectors, f):
        a00 = merge([x[0], y[0]], mode = f, output_shape = (neg_samples, vect_dim))
        a01 = merge([x[0], y[1]], mode = f, output_shape = (neg_samples, vect_dim))
        a11 = merge([x[1], y[1]], mode = f, output_shape = (neg_samples, vect_dim))
        a10 = merge([x[1], y[0]], mode = f, output_shape = (neg_samples, vect_dim))
        r1 = merge([relation_vectors[0], a00], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r2 = merge([relation_vectors[0], a11], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r3 = merge([relation_vectors[1], a01], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
        r4 = merge([relation_vectors[1], a10], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))

        result = merge([r1,r2,r3,r4], mode = lambda X : X[0]+X[1]-X[2]+X[3], output_shape=(neg_samples,))
        return result

    mu = 0;sigma = 0.05
    print mu,sigma
    randn_init_embed = np.random.normal(mu, sigma, (num_entities+1,vect_dim))
    entities_real  = Embedding(output_dim=vect_dim, input_dim=num_entities+1, weights=[randn_init_embed],name = 'entity_embeddings_real', W_regularizer=l2(l2_reg_entities))
    randn_init_embed = np.random.normal(mu, sigma, (num_entities+1,vect_dim))
    entities_im  = Embedding(output_dim=vect_dim, input_dim=num_entities+1, weights=[randn_init_embed],name = 'entity_embeddings_im', W_regularizer=l2(l2_reg_entities))

    randn_init_embed = np.random.normal(mu, sigma, (num_relations,vect_dim))
    print("regularizing complex relation embeddings with l2 penalty of: %5.4f" %l2_reg_relations)
    relations_real = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1, weights=[randn_init_embed], name='relation_embeddings_real', W_regularizer=l2(l2_reg_relations))
    randn_init_embed = np.random.normal(mu, sigma, (num_relations,vect_dim))
    relations_im = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1, weights=[randn_init_embed], name='relation_embeddings_im', W_regularizer=l2(l2_reg_relations))

    entity_vectors_real = entities_real(kb_entities)
    entity_vectors_im =  entities_im(kb_entities)

    entity_negative_vectors_real = entities_real(neg_samples_kb)
    entity_negative_vectors_im = entities_im(neg_samples_kb)

    relation_vectors_real = Flatten()(relations_real(kb_relations))
    relation_vectors_im = Flatten()(relations_im(kb_relations))

    def l2_theo(input):
        entity_vectors_real, entity_vectors_im, relation_vectors_real, relation_vectors_im, entity_negative_vectors_real, entity_negative_vectors_im = input
        all_e1_real = entity_vectors_real[:, 0]
        all_e1_im   = entity_vectors_im[:, 0]
        all_e2_real = entity_vectors_real[:, 1]
        all_e2_im   = entity_vectors_im[:, 1]
        all_ne1_real = entity_negative_vectors_real[:, 0*neg_samples : (0+1)*neg_samples]
        all_ne1_im   = entity_negative_vectors_im[:, 0*neg_samples : (0+1)*neg_samples]
        all_ne2_real = entity_negative_vectors_real[:, 1*neg_samples : (1+1)*neg_samples]
        all_ne2_im   = entity_negative_vectors_im[:, 1*neg_samples : (1+1)*neg_samples]
        denom = opts.batch_size * opts.vect_dim * (1 +neg_samples )
        return (T.sqr(all_e1_real).sum()/denom) + (T.sqr(all_e1_im).sum()/denom) + (T.sqr(all_e2_real).sum()/denom) + (T.sqr(all_e2_im).sum()/denom) + (T.sqr(relation_vectors_real).mean()) + (T.sqr(relation_vectors_im).mean()) + (T.sqr(all_ne1_real).sum()/denom) + (T.sqr(all_ne1_im).sum()/denom) + (T.sqr(all_ne2_real).sum()/denom) + (T.sqr(all_ne2_im).sum()/denom)

    def l2_standard(input):
        entities_real,entities_im,relations_real,relations_im = input
        return (T.sqr(entities_real).mean()) + (T.sqr(entities_im).mean()) + (T.sqr(relations_real).mean()) + (T.sqr(relations_im).mean())   

    reg = merge([entity_vectors_real, entity_vectors_im, relation_vectors_real, relation_vectors_im, entity_negative_vectors_real, entity_negative_vectors_im], mode = l2_theo,  output_shape = ())

    get_cross_1 = get_cross(0, neg_samples)
    get_cross_2 = get_cross(1, neg_samples)

    score_complex = merge([entity_vectors_real, entity_vectors_im, relation_vectors_real, relation_vectors_im], mode = hermitian_product,  output_shape = ())

    score_complex_e1_corrupted = accumulator_e2([entity_vectors_real, entity_vectors_im], [entity_negative_vectors_real, entity_negative_vectors_im],
                                             [relation_vectors_real, relation_vectors_im] ,get_cross_2) 

    score_complex_e2_corrupted = accumulator([entity_vectors_real, entity_vectors_im], [entity_negative_vectors_real, entity_negative_vectors_im],
                                            [relation_vectors_real, relation_vectors_im] ,get_cross_1)  

    return score_complex, score_complex_e1_corrupted, score_complex_e2_corrupted, reg

def getDM_score_joint(kb_entities, kb_relations, neg_samples_kb, relations, opts):
    neg_samples   = opts.neg_samples
    vect_dim      = opts.vect_dim
    num_entities  = opts.num_entities
    num_relations = opts.num_relations
    l2_reg_entities = opts.l2_entity    
    # +1 for the OOV embedding.
    entities  = Embedding(output_dim=vect_dim, input_dim=num_entities+1, init='normal',name = 'entity_embeddings_DM', W_regularizer=l2(l2_reg_entities))

    entity_vectors = entities(kb_entities)
    entity_negative_vectors = entities(neg_samples_kb)
    relation_vectors = Flatten()(relations(kb_relations))


    get_cross_1 = get_cross(0, neg_samples)
    e1_cross_e2_prime = merge([entity_vectors, entity_negative_vectors], mode = get_cross_1, output_shape = (neg_samples, vect_dim))
    e1_cross_e2 = Lambda(cross_e1_e2, output_shape = (vect_dim,))(entity_vectors)

    score_DM = merge([relation_vectors, e1_cross_e2], mode = lambda X : T.batched_dot(X[0], X[1]), output_shape=())
    score_DM_e2_corrupted = merge([relation_vectors, e1_cross_e2_prime], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))

 
    if opts.add_loss:
        get_cross_2 = get_cross(1, neg_samples)
        e1_prime_cross_e2 = merge([entity_vectors, entity_negative_vectors], mode = get_cross_2, output_shape = (neg_samples, vect_dim))
        score_DM_e1_corrupted = merge([relation_vectors, e1_prime_cross_e2], mode = 'dot', output_shape=(neg_samples,), dot_axes=(1,2))
    else:
        score_DM_e1_corrupted = None

    def l2_theo(input):
        entity_vectors, relation_vectors, entity_negative_vectors = input
        all_e1_real = entity_vectors[:, 0]
        all_e2_real = entity_vectors[:, 1]
        all_ne1_real = entity_negative_vectors[:, 0*neg_samples : (0+1)*neg_samples]
        all_ne2_real = entity_negative_vectors[:, 1*neg_samples : (1+1)*neg_samples]
        denom = opts.batch_size * opts.vect_dim * (1 +neg_samples )
        return (T.sqr(all_e1_real).sum()/denom) + (T.sqr(all_e2_real).sum()/denom) + (T.sqr(relation_vectors).mean()) + (T.sqr(all_ne1_real).sum()/denom) + (T.sqr(all_ne2_real).sum()/denom)

    if opts.theo_reg:
        reg = merge([entity_vectors, relation_vectors, entity_negative_vectors], mode = l2_theo,  output_shape = ())
        return score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg
    else:
        return score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted , 0


def neural_model(MF_data, DM_data, aux_features, opts):
    score_MF, score_MF_corrupted = MF_data
    score_DM, score_DM_e2_corrupted = DM_data
    neg_samples = opts.neg_samples

    stacked_scores = merge([score_MF, score_DM], mode = lambda X: T.stack([X[0], X[1]], axis=1), output_shape=(None,2))

    # tile the (None, 4) feature matrix into a (None, 4, neg_samples) feature matrix by repeating it along the 3rd axis and then shuffle 2nd and 3rd dimension to get a (None, neg_samples, 4)
    aux_features_tiled = Lambda(lambda X : T.tile(X.dimshuffle(0,1,'x'), reps=(1,1,neg_samples)).dimshuffle(0,2,1), output_shape = (neg_samples, 4))(aux_features)

    # stack DM score, MF score, and the 4 features in a single (None, 6) matrix
    stacked_inputs = merge([stacked_scores, aux_features], mode= lambda X: T.concatenate([X[0], X[1]], axis=1), output_shape=(None,6))

    # stack (None, neg_samples, 1) DM and MF negative sample data along with (None, neg_samples, 4) 
    stacked_inputs_e2_corrupt = merge([score_MF_corrupted, score_DM_e2_corrupted, aux_features_tiled], lambda X : T.concatenate([X[0].dimshuffle(0,1,'x'), X[1].dimshuffle(0,1,'x'), X[2]], axis=2), output_shape=(None,neg_samples,6))  

    #define the neural network for outputing new corrupted and real scores
    nnet_layer_1 = Dense(10, init='glorot_normal',activation='tanh', name='dense_1')
    nnet_layer_2 = Dense(10, init='glorot_normal',activation='tanh', name='dense_2')
    nnet_layer_3 = Dense(1, init = 'glorot_normal', name='dense_3')

    layers = [nnet_layer_1, nnet_layer_2, nnet_layer_3]
    # create a function to perform forward passes over the neural network
    forward_pass = get_forward_pass(layers)
    forward_pass_distributed = get_forward_pass([TimeDistributed(layer) for layer in layers])

    score_combined = forward_pass(stacked_inputs)
    score_combined_e2_corrupt = Flatten()(forward_pass_distributed(stacked_inputs_e2_corrupt))

    return score_combined, score_combined_e2_corrupt

def featureNet_model(MF_data, DM_data, aux_features, opts):
    score_MF, score_MF_corrupted = MF_data
    score_DM, score_DM_e2_corrupted = DM_data
    neg_samples = opts.neg_samples

    # tile the (None, 4) feature matrix into a (None, 4, neg_samples) feature matrix by repeating it along the 3rd axis and then shuffle 2nd and 3rd dimension to get a (None, neg_samples, 4)
    aux_features_tiled = Lambda(lambda X : T.tile(X.dimshuffle(0,1,'x'), reps=(1,1,neg_samples)).dimshuffle(0,2,1), output_shape = (neg_samples, 4))(aux_features)

    #define the neural network for outputing new corrupted and real scores
    nnet_layer_1 = Dense(10, init='glorot_normal',activation='tanh',name='dense_1')
    nnet_layer_2 = Dense(10, init='glorot_normal',activation='tanh',name='dense_2')
    nnet_layer_3 = Dense(1, init = 'glorot_normal', activation = 'sigmoid', name='dense_3')

    layers = [nnet_layer_1, nnet_layer_2, nnet_layer_3]
    # create a function to perform forward passes over the neural network
    forward_pass = get_forward_pass(layers)
    forward_pass_distributed = get_forward_pass([TimeDistributed(layer) for layer in layers])

    f1 = forward_pass(aux_features)
    f2 = Flatten()(forward_pass_distributed(aux_features_tiled))

    if opts.normalize_score:
        #nnet_layer_std = Dense(1, init='glorot_normal',name='normalize_MF', W_constraint = maxnorm(0.0001), b_constraint = maxnorm(0.0001))
        nnet_layer_std = Dense(1, init='glorot_normal',name='normalize_MF')
        forward_pass_std = get_forward_pass([nnet_layer_std])
        forward_pass_distributed_std = get_forward_pass([TimeDistributed(nnet_layer_std)])
        score_MF = Lambda(lambda X: (X.dimshuffle(0,'x')), output_shape=(1,))(score_MF)#batch_size x 1
        #(None, neg-samples) --> (None, neg-samples, 1)
        score_MF_corrupted = Lambda(lambda X: X.dimshuffle(0,1,'x'), output_shape=(neg_samples,1))(score_MF_corrupted)        
        score_MF = forward_pass_std(score_MF)
        score_MF = Lambda(lambda X: X[:,0], output_shape=())(score_MF)
        score_MF_corrupted = Flatten()(forward_pass_distributed_std(score_MF_corrupted))
      
        #nnet_layer_std_DM = Dense(1, init='glorot_normal',name='normalize_DM', W_constraint = maxnorm(0.0001), b_constraint = maxnorm(0.0001))
        nnet_layer_std_DM = Dense(1, init='glorot_normal',name='normalize_DM')
        forward_pass_std_DM = get_forward_pass([nnet_layer_std_DM])
        forward_pass_distributed_std_DM = get_forward_pass([TimeDistributed(nnet_layer_std_DM)]) 
        score_DM = Lambda(lambda X: X.dimshuffle(0,'x'), output_shape=(1,))(score_DM)
        score_DM_e2_corrupted = Lambda(lambda X: X.dimshuffle(0,1,'x'), output_shape=(neg_samples,1))(score_DM_e2_corrupted)
        score_DM = forward_pass_std_DM(score_DM)
        score_DM = Lambda(lambda X: X[:,0], output_shape=())(score_DM)
        score_DM_e2_corrupted = Flatten()(forward_pass_distributed_std_DM(score_DM_e2_corrupted))
        
    score_MF = merge([score_MF, f1], lambda X: X[0]*(X[1][:,0]), output_shape = (None,), name="alpha_MF")
    score_MF_corrupted = merge([score_MF_corrupted, f2], lambda X: X[0]*X[1], output_shape = (None, neg_samples))

    score_combined = merge([score_MF, score_DM], lambda X: X[0] + X[1], output_shape = (None,)) 
    score_combined_e2_corrupt = merge([score_MF_corrupted, score_DM_e2_corrupted], mode='sum')

    return score_combined, score_combined_e2_corrupt

def adder_model(MF_data, DM_data, opts):
    score_MF, score_MF_corrupted = MF_data
    score_DM, score_DM_e2_corrupted = DM_data
    neg_samples = opts.neg_samples
    print "I am here: adder_model" 
    if opts.static_alpha:#jan28
        print "I am here: alphaMF * MF + DM"
        score_MF = Lambda(lambda X: opts.alphaMF * X[0], output_shape=())(score_MF)
        score_MF_corrupted = Lambda(lambda X: opts.alphaMF * X[:,], output_shape=(neg_samples,))(score_MF_corrupted)

    score_combined = merge([score_MF, score_DM], lambda X: X[0] + X[1], output_shape = ()) 
    score_combined_e2_corrupt = merge([score_MF_corrupted, score_DM_e2_corrupted], mode='sum')

    return score_combined, score_combined_e2_corrupt


def get_max_margin(input):
    score, score_e1_corrupt = input
    net_loss =  T.sum(T.maximum(0,1.0 + score_e1_corrupt - score.dimshuffle(0,'x')), axis=1)
    return 1*net_loss

def get_loss_fn(opts):
    if opts.loss == "ll":
	    print "Using log likelihood based loss!"
	    return get_softmax_approx
    elif opts.loss == "mm":
	    print "using max-margin loss!"
	    return get_max_margin
    elif opts.loss == "logistic":
        print "using logistic loss!"
        return get_logistic 


def build_joint_model(opts, combine_func, adder_model=False, add_loss = False):
    neg_samples = opts.neg_samples
    optimizer = opts.optimizer
    vect_dim  = opts.vect_dim
    num_relations = opts.num_relations
    l2_reg_relation = opts.l2_relation   

    # (e1,e2) for distMult
    kb_entities     = Input(shape=(2,), dtype='int32', name='kb_entities')
    # r id for both distMult and Matrix Factorization
    kb_relations_DM    = Input(shape=(1,), dtype='int32', name='kb_relations_DM')
    kb_relations_MF    = Input(shape=(1,), dtype='int32', name='kb_relations_MF')
    # (e1,e2) for MF (represented as a single ID)
    kb_entity_pairs = Input(shape=(1,), dtype='int32', name='kb_entity_pairs')
    # negative samples for DM
    neg_samples_kb_MF  = Input(shape=(neg_samples,), dtype = 'int32', name='kb_neg_examples_MF')
    # negative samples for MF
    if add_loss:
        neg_samples_kb_DM  = Input(shape=((neg_samples)*2,), dtype = 'int32', name='kb_neg_examples_DM')
    else:
        neg_samples_kb_DM  = Input(shape=(neg_samples,), dtype = 'int32', name='kb_neg_examples_DM')

    if not adder_model:
        aux_features = Input(shape=(4,), dtype='float32', name='auxiliary features')


    flag_hybrid_naive = (opts.model == "FE" or opts.model == "DMFE")
    if opts.model == "DME":
        score_E, score_E_e1_corrupted, score_E_e2_corrupted, reg_E = getE_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, opts)
        relations_DM = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_DM')
        score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg_DM = getDM_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, relations_DM, opts)
        alpha = 1.0;beta=1.0#0.00001#0.0000001
        score_E =  merge([score_DM, score_E], mode = lambda X: alpha*X[0]+beta*X[1], output_shape=())
        score_E_e2_corrupted =  merge([score_DM_e2_corrupted, score_E_e2_corrupted], mode = lambda X: alpha*X[0]+beta*X[1], output_shape=(neg_samples,)) 

        if opts.loss == "logistic":
            log_likelihood_kb = merge([score_E, score_E_e2_corrupted], mode= get_logistic, output_shape=(1,))
        else:
            log_likelihood_kb = merge([score_E, score_E_e2_corrupted], mode= get_softmax_approx, output_shape=(1,))

        if reg_E:
            print "E Model Regularization!", opts.theo_reg
            log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_E])
        if reg_DM:
            print "DM Model Regularization!", opts.theo_reg
            log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_DM])


    elif flag_hybrid_naive:
        relations_MF = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings')

        score_MF, score_MF_corrupted, reg_MF = getMF_score_joint(kb_entity_pairs, kb_relations_MF, neg_samples_kb_MF, relations_MF, opts)
        score_E, score_E_e1_corrupted, score_E_e2_corrupted = getE_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, opts)

        if add_loss:
            print "add loss of MF and E model!!"
            log_likelihood_kb_MF = merge([score_MF, score_MF_corrupted], mode= get_loss_fn(opts), output_shape=(1,))
            log_likelihood_kb_E_2 = merge([score_E, score_E_e2_corrupted], mode= get_loss_fn(opts), output_shape=(1,))
            log_likelihood_kb_E_1 = merge([score_E, score_E_e1_corrupted], mode= get_loss_fn(opts), output_shape=(1,))
            log_likelihood_kb_E = merge([log_likelihood_kb_E_1 , log_likelihood_kb_E_2], lambda X: X[0] + X[1], output_shape = (1,))
            
        if opts.model == "DMFE":
            print "add DM loss"
            relations_DM = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_DM')
            if opts.use_complex:
                score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg_DM = getComplexDM_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, relations_DM, opts)
            else:
                score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg_DM = getDM_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, relations_DM, opts)
            if add_loss:
                log_likelihood_kb_DM_1 = merge([score_DM, score_DM_e1_corrupted], mode= get_loss_fn(opts), output_shape=(1,))
                log_likelihood_kb_DM_2 = merge([score_DM, score_DM_e2_corrupted], mode= get_loss_fn(opts), output_shape=(1,))
                log_likelihood_kb_DM = merge([log_likelihood_kb_DM_1 , log_likelihood_kb_DM_2], lambda X: X[0] + X[1], output_shape = (1,))
                log_likelihood_kb_E =  merge([log_likelihood_kb_DM, log_likelihood_kb_E], mode = lambda X: X[0]+X[1], output_shape=(1,))

            else:
                score_E =  merge([score_DM, score_E], mode = lambda X: X[0]+X[1], output_shape=())
                score_E_e2_corrupted =  merge([score_DM_e2_corrupted, score_E_e2_corrupted], mode = lambda X: X[0]+X[1], output_shape=(neg_samples,))


        if add_loss:
            print "Add loss model"
            log_likelihood_kb =  merge([log_likelihood_kb_MF, log_likelihood_kb_E], mode = lambda X: X[0]+X[1], output_shape=(1,))

        else:
            score_combined = merge([score_MF, score_E], mode = lambda X: X[0]+X[1], output_shape=(1,))
            score_combined_e2_corrupt = merge([score_MF_corrupted, score_E_e2_corrupted], mode = lambda X: X[0]+X[1], output_shape = (neg_samples,)) 

            log_likelihood_kb = merge([score_combined, score_combined_e2_corrupt], mode= get_softmax_approx, output_shape=(1,))            

        if reg_MF:
            print "MF Model Regularization!", opts.l2_relation_MF
            #log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_MF])
            log_likelihood_kb = Lambda(lambda x: x[0] + opts.l2_relation_MF * x[1], output_shape = (1,))([log_likelihood_kb, reg_MF])
        if reg_DM:
            print "DM Model Regularization!", opts.theo_reg
            log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_DM])
               
    else:
        relations_DM = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_DM')

        if 0:
            relations_MF = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_MF', W_regularizer=l2(opts.l2_relation_MF))
            print("regularizing MF relation embeddings with l2 penalty of: %5.4f" %opts.l2_relation_MF)
        else:
            relations_MF = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='normal', name='relation_embeddings_MF')
            #relations_MF = Embedding(output_dim=vect_dim, input_dim=num_relations, input_length=1,init='zeros', name='relation_embeddings_MF',trainable = False)


        score_MF, score_MF_corrupted, reg_MF = getMF_score_joint(kb_entity_pairs, kb_relations_MF, neg_samples_kb_MF, relations_MF, opts)

        if opts.use_complex:#
            score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg_DM = getComplexDM_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, relations_DM, opts)
        else:
            score_DM, score_DM_e1_corrupted, score_DM_e2_corrupted, reg_DM = getDM_score_joint(kb_entities, kb_relations_DM, neg_samples_kb_DM, relations_DM, opts)

        if add_loss:
            if opts.loss == "logistic":
                log_likelihood_kb_MF = merge([score_MF, score_MF_corrupted], mode= get_logistic, output_shape=(1,))
                log_likelihood_kb_DM = merge([score_DM, score_DM_e1_corrupted,score_DM_e2_corrupted], mode= get_logistic, output_shape=(1,))
            else:
                log_likelihood_kb_MF = merge([score_MF, score_MF_corrupted], mode= get_softmax_approx, output_shape=(1,))
                log_likelihood_kb_DM_1 = merge([score_DM, score_DM_e1_corrupted], mode= get_softmax_approx, output_shape=(1,))
                log_likelihood_kb_DM_2 = merge([score_DM, score_DM_e2_corrupted], mode= get_softmax_approx, output_shape=(1,))
                log_likelihood_kb_DM = merge([log_likelihood_kb_DM_1 , log_likelihood_kb_DM_2], lambda X: X[0] + X[1], output_shape = (1,))
            if reg_MF:
                print "MF Model Regularization!", opts.l2_relation_MF
                #log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_MF])
                log_likelihood_kb_MF = Lambda(lambda x: x[0] + opts.l2_relation_MF * x[1], output_shape = (1,))([log_likelihood_kb_MF, reg_MF])
            if reg_DM:
                print "DM Model Regularization!", opts.theo_reg
                log_likelihood_kb_DM = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb_DM, reg_DM])
            log_likelihood_kb    = merge([log_likelihood_kb_MF , log_likelihood_kb_DM], lambda X: X[0] + X[1], output_shape = (1,))

        else:
            print "Sameer Singh's Hybrid model!!"
            score_combined = merge([score_MF, score_DM], mode = lambda X: X[0]+X[1], output_shape=(1,))
            score_combined_e2_corrupt = merge([score_MF_corrupted, score_DM_e2_corrupted], mode = lambda X: X[0]+X[1], output_shape = (neg_samples,)) 
            log_likelihood_kb = merge([score_combined, score_combined_e2_corrupt], mode=get_loss_fn(opts), output_shape=(1,))
            if reg_MF:
                print "MF Model Regularization!", opts.l2_relation_MF
                #log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_MF])
                log_likelihood_kb = Lambda(lambda x: x[0] + opts.l2_relation_MF * x[1], output_shape = (1,))([log_likelihood_kb, reg_MF])

            if reg_DM:
                print "DM Model Regularization!", opts.theo_reg
                log_likelihood_kb = Lambda(lambda x: x[0] + opts.theo_reg * x[1], output_shape = (1,))([log_likelihood_kb, reg_DM]) 


    if optimizer=='Adagrad':
        if opts.loss == "logistic":
            print "Clipnorm = 1"
            alg = Adagrad(lr=opts.lr, clipnorm=1)
        else:
            alg = Adagrad(lr=opts.lr)
    elif optimizer=='RMSprop':
        alg = RMSprop(lr=opts.lr)
    else:
        print("This optimizer is currently not supported. Modify models.py if you wish to add it")
        sys.exit(1)

    if opts.model == "DME":
        model = Model(input=[kb_entities, kb_relations_DM, neg_samples_kb_DM], output=log_likelihood_kb)
    elif not adder_model:
        model = Model(input=[kb_entities, kb_entity_pairs, kb_relations_DM, kb_relations_MF, aux_features, neg_samples_kb_DM, neg_samples_kb_MF], output=log_likelihood_kb)
    else:
        model = Model(input=[kb_entities, kb_entity_pairs, kb_relations_DM, kb_relations_MF, neg_samples_kb_DM, neg_samples_kb_MF], output=log_likelihood_kb)
        
    model.compile(loss = lossFn, optimizer=alg)
        
    return model

